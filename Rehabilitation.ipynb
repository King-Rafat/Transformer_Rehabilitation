{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "555c17f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from GCN.data_processing import Data_Loader\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import csv\n",
    "import numpy as np\n",
    "import io\n",
    "import os, random\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image,ImageOps\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from torch import einsum\n",
    "from argparse import ArgumentParser\n",
    "from core.models.curvenet_cls import CurveNet\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "random_seed = 420 #for reproducibility\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torchsummary import summary\n",
    "import colorama\n",
    "from colorama import Fore, Back, Style\n",
    "colorama.init(autoreset=True)\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2f2d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  4 19:49:30 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3a2824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                               |                      |               MIG M. |True\n",
      "\n",
      "|===============================+======================+======================|1\n",
      "\n",
      "NVIDIA GeForce GTX 1650|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "\n",
      "| N/A   68C    P8     1W /  N/A |    367MiB /  4096MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "# print(torch.cuda.current_device())\n",
    "# print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc51fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      5212    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6648    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A      6880    C+G   ...ype for Desktop\\Skype.exe    N/A      |"
     ]
    }
   ],
   "source": [
    "# print(len(np.unique(train_y)))\n",
    "# print(np.unique(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75418ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|    0   N/A  N/A      8204    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     10012    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10200    C+G   ...861\\Grammarly.Desktop.exe    N/A      |\n"
     ]
    }
   ],
   "source": [
    "# print(len(train_x))\n",
    "# print(len(train_y))\n",
    "# print(len(test_x))\n",
    "# print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2314d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    0   N/A  N/A     10840    C+G   ...v1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12664    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     12936    C+G   ...me\\Application\\chrome.exe    N/A      |\n"
     ]
    }
   ],
   "source": [
    "# train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca947bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    0   N/A  N/A     13296    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13368    C+G   ...938.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     13924    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14592    C+G   ...ray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     15144    C+G   ...ype for Desktop\\Skype.exe    N/A      |\n",
      "|    0   N/A  N/A     15840    C+G   ...ation\\Adaware-Privacy.exe    N/A      |\n",
      "|    0   N/A  N/A     16576    C+G   ...938.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     17400    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18200    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |"
     ]
    }
   ],
   "source": [
    "# train_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753ec374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|    0   N/A  N/A     18576    C+G   ...938.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     20024    C+G   ...938.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     23484    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor(train_x)\n",
    "# x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003b1b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.create_embedding_fn()\n",
    "\n",
    "    def create_embedding_fn(self):\n",
    "        embed_fns = []\n",
    "        d = self.kwargs['input_dims']\n",
    "        out_dim = 0\n",
    "        if self.kwargs['include_input']:\n",
    "            embed_fns.append(lambda x : x)\n",
    "            out_dim += d\n",
    "\n",
    "        max_freq = self.kwargs['max_freq_log2']\n",
    "        N_freqs = self.kwargs['num_freqs']\n",
    "\n",
    "        if self.kwargs['log_sampling']:\n",
    "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
    "        else:\n",
    "            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
    "\n",
    "        for freq in freq_bands:\n",
    "            for p_fn in self.kwargs['periodic_fns']:\n",
    "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n",
    "                out_dim += d\n",
    "\n",
    "        self.embed_fns = embed_fns\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def embed(self, inputs):\n",
    "        normalized = torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
    "        return normalized\n",
    "    \n",
    "def get_embedder(multires = 10, i=0):\n",
    "    if i == -1:\n",
    "        return nn.Identity(), 1\n",
    "\n",
    "    embed_kwargs = {\n",
    "                'include_input' : True,\n",
    "                'input_dims' : 1,\n",
    "                'max_freq_log2' : multires-1,\n",
    "                'num_freqs' : multires,\n",
    "                'log_sampling' : True,\n",
    "                'periodic_fns' : [torch.sin, torch.cos],\n",
    "    }\n",
    "\n",
    "    embedder_obj = Embedder(**embed_kwargs)\n",
    "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
    "    return embed, embedder_obj.out_dim\n",
    "\n",
    "embeder = get_embedder()[0]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16783b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "# GELU -> Gaussian Error Linear Units\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RemixerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        seq_len,\n",
    "        causal = False,\n",
    "        bias = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.proj_in = nn.Linear(dim, 2 * dim, bias = bias)\n",
    "        self.mixer = nn.Parameter(torch.randn(seq_len, seq_len))\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.))\n",
    "        self.proj_out = nn.Linear(dim, dim, bias = bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mixer, causal, device = self.mixer, self.causal, x.device\n",
    "        x, gate = self.proj_in(x).chunk(2, dim = -1)\n",
    "        x = F.gelu(gate) * x\n",
    "\n",
    "        if self.causal:\n",
    "            seq = x.shape[1]\n",
    "            mask_value = -torch.finfo(x.dtype).max\n",
    "            mask = torch.ones((seq, seq), device = device, dtype=torch.bool).triu(1)\n",
    "            mixer = mixer[:seq, :seq]\n",
    "            mixer = mixer.masked_fill(mask, mask_value)\n",
    "\n",
    "        mixer = mixer.softmax(dim = -1)\n",
    "        mixed = einsum('b n d, m n -> b m d', x, mixer)\n",
    "\n",
    "        alpha = self.alpha.sigmoid()\n",
    "        out = (x * mixed) * alpha + (x - mixed) * (1 - alpha)\n",
    "\n",
    "        return self.proj_out(out)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: `embeddings`, shape (batch, max_len, d_model)\n",
    "        Returns:\n",
    "            `encoder input`, shape (batch, max_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(f'Attention:: {dim} - {heads} - {dim_head} - {dropout}')\n",
    "\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(dim,0.1,128)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.pos_embedding(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        # print('\\n')\n",
    "        # print(f'Transformers:: {dim} - {depth} - {heads} - {dim_head} - {mlp_dim}')\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "                #PreNorm(dim, RemixerBlock(dim,17))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, swap = False):\n",
    "        if swap: # for the self.transformer(x,swap = True)\n",
    "            b, t, n , c = x.size() \n",
    "        for idx, (attn, ff) in enumerate(self.layers):\n",
    "            if swap: # for the self.transformer(x,swap = True)\n",
    "                if idx % 2 == 0:\n",
    "                    #* attention along with all timesteps(frames) for each point(landmark)\n",
    "                    x = rearrange(x, \"b t n c -> (b n) t c\")\n",
    "                else:\n",
    "                    #* attention to all points(landmarks) in each timestep(frame)\n",
    "                    x = rearrange(x, \"b t n c -> (b t) n c\")\n",
    "            x = attn(x) + x  # skip connections\n",
    "            x = ff(x) + x    # skip connections\n",
    "            \n",
    "            # Now return the input x to its original formation\n",
    "            if swap: # for the self.transformer(x,swap = True)\n",
    "                if idx % 2 == 0:\n",
    "                    x = rearrange(x, \"(b n) t c -> b t n c\", b = b)\n",
    "                else:\n",
    "                    x = rearrange(x, \"(b t) n c -> b t n c\", b = b)\n",
    "                \n",
    "        return x\n",
    "\n",
    "\n",
    "class TemporalModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TemporalModel,self).__init__()\n",
    "                \n",
    "        self.encoder  =  CurveNet() # curve aggregation, needed for Point Clouds Shape Analysis. \n",
    "        self.downsample = nn.Sequential(\n",
    "                            nn.Conv1d(25, 32, kernel_size=1, bias=False),\n",
    "                            nn.BatchNorm1d(32),\n",
    "                            # nn.Dropout(p=0.25), #* NEW\n",
    "                            #nn.ReLU(inplace=True),\n",
    "                            #nn.Conv1d(128, 32, kernel_size=1, bias=False),\n",
    "                            #nn.BatchNorm1d(32),\n",
    "                            )\n",
    "        \n",
    "        self.transformer = Transformer(256, 6, 4, 256//4, 256 * 2, 0.1)\n",
    "        self.time = Transformer(256, 3, 4, 256//4, 256 * 2, 0.1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256,1),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,t,n,c = x.size()\n",
    "    \n",
    "        x = rearrange(x, \"b t n c -> (b t) c n\")\n",
    "        x = rearrange(self.dropout(self.encoder(x)), \"b c n -> b n c\") \n",
    "        x = self.downsample(x).view(b,t,32,-1) #b t 32 c\n",
    "        x = self.transformer(x,swap = True).view(b,t,-1,256).mean(2)\n",
    "        x = self.time(x).mean(1)\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "min_xyz = np.array([0.06372425, 0.05751023, -0.08976112]).reshape(1,1,3)\n",
    "max_xyz = np.array([0.63246971, 1.01475966, 0.14436169]).reshape(1,1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d953751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, nr = 80, n_dim= 117, timesteps= 240):\n",
    "    f = open('./UI-PRMD/'+path+'/Data_Correct.csv')\n",
    "    csv_f = csv.reader(f)\n",
    "    Correct_X = list(csv_f)\n",
    "\n",
    "    # Convert the input sequences into numpy arrays\n",
    "    train_input1 = np.asarray(Correct_X, dtype = float)\n",
    "    correct_input = np.zeros((nr,timesteps,n_dim))\n",
    "    for i in range(len(train_input1)//n_dim):\n",
    "           correct_input[i,:,:] = np.transpose(train_input1[n_dim*i:n_dim*(i+1),:])\n",
    "    \n",
    "    f = open('./UI-PRMD/'+path+'/Labels_Correct.csv')\n",
    "    csv_f = csv.reader(f)\n",
    "    Correct_Y = list(csv_f)\n",
    "    \n",
    "    # Convert the input labels into numpy arrays\n",
    "    correct_label = np.asarray(Correct_Y, dtype = float)\n",
    "    \n",
    "    f = open('./UI-PRMD/'+path+'/Data_Incorrect.csv')    \n",
    "    csv_f = csv.reader(f)\n",
    "    Incorrect_X = list(csv_f)\n",
    "\n",
    "    # Convert the input sequences into numpy arrays\n",
    "    test_input1 = np.asarray(Incorrect_X)\n",
    "    n_dim = 117\n",
    "    incorrect_input = np.zeros((nr,timesteps,n_dim))\n",
    "    for i in range(len(test_input1)//n_dim):\n",
    "          incorrect_input[i,:,:] = np.transpose(test_input1[n_dim*i:n_dim*(i+1),:])\n",
    "            \n",
    "    f = open('./UI-PRMD/'+path+'/Labels_Incorrect.csv')\n",
    "    csv_f = csv.reader(f)\n",
    "    Incorrect_Y = list(csv_f)\n",
    "    \n",
    "    # Convert the input labels into numpy arrays\n",
    "    incorrect_label = np.asarray(Incorrect_Y, dtype = float)\n",
    "    \n",
    "    return correct_input, correct_label, incorrect_input, incorrect_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec067923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "def performance_metrics(test_y, y_pred):\n",
    "    test_dev = abs(test_y-y_pred)\n",
    "#     mean_abs_dev = torch.mean(test_dev)\n",
    "    mae = mean_absolute_error(test_y, y_pred)\n",
    "    rms_dev = sqrt(mean_squared_error(y_pred, test_y))\n",
    "    mse = mean_squared_error(test_y,y_pred) \n",
    "    mape = mean_absolute_percentage_error(test_y, y_pred)\n",
    "    return mae, rms_dev, mse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a2dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,training_generator,test_generator, file, data_loader = None):\n",
    "    \n",
    "    con = []      \n",
    "    net = TemporalModel()\n",
    "    net.cuda()\n",
    "    eval_loss = 500\n",
    "    lr = 0.0001\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr,weight_decay= 0.0)\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000], gamma=0.1)\n",
    "    loss_func = torch.nn.HuberLoss(reduction='mean', delta= 0.1)\n",
    "    start_time = time.time()\n",
    "    best_accuracy = 500\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pred_label = []\n",
    "        true_label = []\n",
    "        number_batch = 0\n",
    "        for x, y in tqdm(training_generator, desc=f\"Epoch {epoch}/{epochs-1}\", ncols=60):\n",
    "            if torch.cuda.device_count() > 0:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            x = torch.unsqueeze(x, 1) \n",
    "#             print(x.size())\n",
    "            b,d,t,n,c = x.size()\n",
    "            x = x.view(-1,t,n,c)\n",
    "            pred = net(x)\n",
    "            loss = loss_func(pred,y)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred_label.extend(pred.flatten())\n",
    "            true_label.extend(y.flatten())\n",
    "            number_batch += 1\n",
    "#             lr = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "#         lr_scheduler.step()\n",
    "        \n",
    "#         print(true_label)\n",
    "#         print(pred_label)\n",
    "#         pred_label = torch.cat(pred_label,0)\n",
    "#         true_label = torch.cat(true_label,0)\n",
    "#         train_accuracy = torch.sum(pred_label == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "        pred_label = torch.tensor(pred_label)\n",
    "        true_label = torch.tensor(true_label)\n",
    "        mae, rms_dev, mse, mape = performance_metrics(true_label, pred_label)\n",
    "    \n",
    "        output('Epoch: ' + 'train ' + str(epoch) + \n",
    "              ', mae ' + str(mae) + \n",
    "              ', rms_dev: ' + str(rms_dev) +\n",
    "              ', mse: ' + str(mse) +\n",
    "                ', mape: ' + str(mape)\n",
    "              )\n",
    "        print('Epoch: ' + 'train ' + str(epoch) + \n",
    "              ', mae ' + str(mae) + \n",
    "              ', rms_dev: ' + str(rms_dev) +\n",
    "              ', mse: ' + str(mse) +\n",
    "              ', mape: ' + str(mape)\n",
    "              )\n",
    "        net.eval()\n",
    "        \n",
    "        pred_label = []\n",
    "        pred_avg   = []\n",
    "        true_label = []\n",
    "        with torch.no_grad():\n",
    "          for x, y in tqdm(test_generator, desc=f\"Epoch {epoch}/{epochs-1}\", ncols=60):\n",
    "\n",
    "              if torch.cuda.device_count() > 0:\n",
    "                  x = x.cuda()\n",
    "                  y = y.cuda()\n",
    "              x = torch.unsqueeze(x, 1)\n",
    "              b,d,t,n,c = x.size()\n",
    "              x = x.view(-1,t,n,c)\n",
    "              pred_y    = net(x)\n",
    "            ###commented to convert round\n",
    "#               pred_mean = (pred_y.view(b,d).mean(1,keepdim = True) >= 0.5).float().cpu().detach()\n",
    "#               pred_y    = ((pred_y).view(b,d).mean(1,keepdim = True) >= 0.5).float().cpu().detach()\n",
    "              \n",
    "              pred_label.extend(pred_y)\n",
    "              true_label.extend(y)\n",
    "# #           pred_avg   = torch.cat(pred_avg,0)  \n",
    "# #           true_label = torch.cat(true_label,0)\n",
    "          pred_label = torch.tensor(pred_label)\n",
    "          true_label = torch.tensor(true_label)\n",
    "          loss_e = loss_func(pred_label,true_label)\n",
    "          pred_label = torch.unsqueeze(pred_label,1)\n",
    "          true_label = torch.unsqueeze(true_label,1)\n",
    "          \n",
    "          if data_loader is not None:\n",
    "              pred_label = data_loader.sc2.inverse_transform(pred_label)\n",
    "              true_label = data_loader.sc2.inverse_transform(true_label)\n",
    "                \n",
    "#           print(pred_label)\n",
    "#           print(true_label)\n",
    "          mae, rms_dev, mse, mape = performance_metrics(true_label, pred_label)\n",
    "#           test_accuracy = torch.sum(pred_label == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "#           test_avg      = torch.sum(pred_avg   == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "          con.append([epoch,mae])\n",
    "          output('test accuracy: ' + \n",
    "              'mae ' + str(mae) + \n",
    "              ', rms_dev: ' + str(rms_dev) +\n",
    "              ', mse: ' + str(mse) +\n",
    "              ', mape: ' + str(mape) +\n",
    "                 ', loss' + str(loss_e)\n",
    "              )\n",
    "          print(Fore.GREEN + 'test accuracy: ' + \n",
    "              'mae ' + str(mae) + \n",
    "              ', rms_dev: ' + str(rms_dev) +\n",
    "              ', mse: ' + str(mse) +\n",
    "              ', mape: ' + str(mape)+\n",
    "                ', loss' + str(loss_e)\n",
    "              )\n",
    "          print()\n",
    "          if mae < best_accuracy:\n",
    "              filepath = f\"{file}-{epoch:}-{loss}-{mae}.pt\"\n",
    "              torch.save(net.state_dict(), filepath)\n",
    "            #   torch.save(net, filepath)\n",
    "            #   test_frames(f'{test_accuracy}={test_f}')\n",
    "              output('Best Results Achieved!!!!!!!!!!!!')\n",
    "              print(\"Best Results Achieved!!!!!!!!!!!!\")\n",
    "              best_accuracy = mae\n",
    "\n",
    "        net.train()\n",
    "        \n",
    "        output(f\"ETA Per Epoch:{(time.time() - start_time) / (epoch + 1)}\")\n",
    "        # print(f\"ETA Per Epoch:{(time.time() - start_time) / (epoch + 1)}\")\n",
    "\n",
    "    print(\"Lowest loss: \", eval_loss)\n",
    "    \n",
    "    \n",
    "# image_size = 48\n",
    "# label_path = \"labels\"\n",
    "# data = \"npy\"\n",
    "\n",
    "# sometimes = lambda aug: va.Sometimes(0.5, aug)\n",
    "# seq = va.Sequential([\n",
    "#     va.RandomCrop(size=(image_size, image_size)),       \n",
    "#     sometimes(va.HorizontalFlip()),              \n",
    "# ])\n",
    "\n",
    "\n",
    "# label_path = \"labels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69436be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_generator, data_loader, net):\n",
    "    net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    pred_label = []\n",
    "    pred_avg   = []\n",
    "    true_label = []\n",
    "    with torch.no_grad():\n",
    "      for x, y in tqdm(test_generator, ncols=60):\n",
    "          if torch.cuda.device_count() > 0:\n",
    "              x = x.cuda()\n",
    "              y = y.cuda()\n",
    "          x = torch.unsqueeze(x, 1)\n",
    "          b,d,t,n,c = x.size()\n",
    "          x = x.view(-1,t,n,c)\n",
    "          pred_y = net(x)\n",
    "        ###commented to convert round\n",
    "#               pred_mean = (pred_y.view(b,d).mean(1,keepdim = True) >= 0.5).float().cpu().detach()\n",
    "#               pred_y    = ((pred_y).view(b,d).mean(1,keepdim = True) >= 0.5).float().cpu().detach()\n",
    "\n",
    "          pred_label.extend(pred_y)\n",
    "          true_label.extend(y)\n",
    "# #           pred_avg   = torch.cat(pred_avg,0)  \n",
    "# #           true_label = torch.cat(true_label,0)\n",
    "      pred_label = torch.tensor(pred_label)\n",
    "      true_label = torch.tensor(true_label)\n",
    "#       loss_e = loss_func(pred_label,true_label)\n",
    "      pred_label = torch.unsqueeze(pred_label,1)\n",
    "      true_label = torch.unsqueeze(true_label,1)\n",
    "\n",
    "      if data_loader is not None:\n",
    "          pred_label = data_loader.sc2.inverse_transform(pred_label)\n",
    "          true_label = data_loader.sc2.inverse_transform(true_label)\n",
    "\n",
    "#           print(pred_label)\n",
    "#           print(true_label)\n",
    "      mae, rms_dev, mse, mape = performance_metrics(true_label, pred_label)\n",
    "#           test_accuracy = torch.sum(pred_label == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "#           test_avg      = torch.sum(pred_avg   == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "#       con.append([epoch,mae])\n",
    "      output('test accuracy: ' + \n",
    "          'mae ' + str(mae) + \n",
    "          ', rms_dev: ' + str(rms_dev) +\n",
    "          ', mse: ' + str(mse) +\n",
    "          ', mape: ' + str(mape)\n",
    "          )\n",
    "      print(Fore.GREEN + 'test accuracy: ' + \n",
    "          'mae ' + str(mae) + \n",
    "          ', rms_dev: ' + str(rms_dev) +\n",
    "          ', mse: ' + str(mse) +\n",
    "          ', mape: ' + str(mape)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "caffd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x, y, net, data_loader):\n",
    "    net.cuda()\n",
    "    net.eval()\n",
    "    x = torch.unsqueeze(x,0)\n",
    "    y = torch.unsqueeze(y,0)\n",
    "    pred_label = []\n",
    "    pred_avg   = []\n",
    "    true_label = []\n",
    "    with torch.no_grad():\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        b,d,t,n,c = x.size()\n",
    "        x = x.view(-1,t,n,c)\n",
    "        pred_y = net(x)\n",
    "        pred_label.extend(pred_y)\n",
    "        true_label.extend(y)\n",
    "\n",
    "        pred_label = torch.tensor(pred_label)\n",
    "        true_label = torch.tensor(true_label)\n",
    "#       loss_e = loss_func(pred_label,true_label)\n",
    "        pred_label = torch.unsqueeze(pred_label,1)\n",
    "        true_label = torch.unsqueeze(true_label,1)\n",
    "\n",
    "        if data_loader is not None:\n",
    "            pred_label = data_loader.sc2.inverse_transform(pred_label)\n",
    "            true_label = data_loader.sc2.inverse_transform(true_label)\n",
    "        print(\"Actual Score: \", true_label)\n",
    "        print(\"Predicted Score: \", pred_label)\n",
    "        mae, rms_dev, mse, mape = performance_metrics(true_label, pred_label)\n",
    "#           test_accuracy = torch.sum(pred_label == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "#           test_avg      = torch.sum(pred_avg   == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
    "#       con.append([epoch,mae])\n",
    "        output('test accuracy: ' + \n",
    "          'mae ' + str(mae) + \n",
    "          ', rms_dev: ' + str(rms_dev) +\n",
    "          ', mse: ' + str(mse) +\n",
    "          ', mape: ' + str(mape)\n",
    "          )\n",
    "        print(Fore.GREEN + 'test accuracy: ' + \n",
    "          'mae ' + str(mae) + \n",
    "          ', rms_dev: ' + str(rms_dev) +\n",
    "          ', mse: ' + str(mse) +\n",
    "          ', mape: ' + str(mape)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3fcb4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "class prmd_scaler():\n",
    "    def __init__(self, y_train):\n",
    "        self.y_train = y_train\n",
    "        self.sc2 = MinMaxScaler()\n",
    "        self.scaled_y = self.preprocessing()\n",
    "    def preprocessing(self):\n",
    "        y = self.sc2.fit_transform(self.y_train)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f8577365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Add the arguments\n",
    "    ####################Parser not working###########################\n",
    "    ex = \"Kimore_ex5\"\n",
    "    lr = 0.0001\n",
    "    epoch = 2000\n",
    "    batch_size = 5\n",
    "    use_type = \"viz\" #train or test or inference\n",
    "    ####################Parser not working###########################\n",
    "\n",
    "    # my_parser.add_argument('--ex', type=str, default=\"Kimore_ex5\",\n",
    "    #                        help='the name of exercise.', required=True)\n",
    "    # my_parser.add_argument('--lr', type=int, default= 0.0001,\n",
    "    #                        help='initial learning rate for optimizer.')\n",
    "    # my_parser.add_argument('--epoch', type=int, default= 1000,\n",
    "    #                        help='number of epochs to train.')\n",
    "    # my_parser.add_argument('--batch_size', type=int, default= 10,\n",
    "    #                        help='training batch size.')\n",
    "    # args = my_parser.parse_args()\n",
    "    global output\n",
    "    def output(s):\n",
    "        with open(f\"log_{ex}\",\"a\") as f:\n",
    "            f.write(str(s) + \"\\n\")\n",
    "            \n",
    "    if \"Kimore\" in ex:\n",
    "        data_loader = Data_Loader(ex) \n",
    "        train_x, test_x, train_y, test_y = train_test_split(data_loader.scaled_x, data_loader.scaled_y, test_size=0.2, \n",
    "                                                            random_state = random_seed)\n",
    "        \n",
    "    else:\n",
    "        nr = 80\n",
    "        Correct_data, Correct_label, Incorrect_data, Incorrect_label = load_data(ex)\n",
    "        print(Correct_data.shape[0])\n",
    "        print(Correct_label.shape)\n",
    "        trainidx1 = random.sample(range(0,Correct_data.shape[0]),int(nr*0.8))\n",
    "        trainidx2 = random.sample(range(0,Incorrect_data.shape[0]),int(nr*0.8))\n",
    "        valididx1 = np.setdiff1d(np.arange(0,nr,1),trainidx1)\n",
    "        valididx2 = np.setdiff1d(np.arange(0,nr,1),trainidx2)\n",
    "        # Training set: data and labels\n",
    "        train_x = np.concatenate((Correct_data[trainidx1,:,:],Incorrect_data[trainidx2,:,:]))\n",
    "        train_y = np.concatenate((np.squeeze(Correct_label[trainidx1]),np.squeeze(Incorrect_label[trainidx2])))\n",
    "\n",
    "        # Validation set: data and labels\n",
    "        test_x = np.concatenate((Correct_data[valididx1,:,:],Incorrect_data[valididx2,:,:]))\n",
    "#         print(valid_x.shape, 'validation data')\n",
    "        test_y = np.concatenate((np.squeeze(Correct_label[valididx1]),np.squeeze(Incorrect_label[valididx2])))\n",
    "        \n",
    "#         print(valid_y.shape, 'validation labels')\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "    print(test_x.shape)\n",
    "    print(test_y.shape)\n",
    "    train_x = torch.Tensor(train_x)\n",
    "    train_y = torch.Tensor(train_y)\n",
    "    test_x  = torch.Tensor(test_x)\n",
    "    test_y = torch.Tensor(test_y)\n",
    "    if \"UI\" in ex:\n",
    "        train_x = torch.unsqueeze(train_x, 3)\n",
    "        train_y = torch.unsqueeze(train_y, 1)\n",
    "        train_x = train_x.expand(train_x.size(dim = 0),train_x.size(dim = 1),train_x.size(dim = 2),3)\n",
    "        print(train_x.size())\n",
    "        test_x = torch.unsqueeze(test_x,3)\n",
    "        test_y = torch.unsqueeze(test_y, 1)\n",
    "        test_x= test_x.expand(test_x.size(dim = 0), test_x.size(dim = 1), test_x.size(dim = 2),3)\n",
    "        print(test_x.size())\n",
    "#         data_loader = prmd_scaler(train_y)\n",
    "#         train_y = data_loader.scaled_y\n",
    "# #         test_y = data_loader.scaled_yt\n",
    "# #         print(test_y[:10])\n",
    "#         train_y = torch.Tensor(train_y)\n",
    "#         test_y = torch.Tensor(test_y)\n",
    "        print(train_y.size())\n",
    "        print(test_y.size())\n",
    "        \n",
    "        \n",
    "    dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "    training_generator = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "    dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "    test_generator = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "    if use_type =='train':\n",
    "        current_path = './models_UI/'\n",
    "        train(epoch,training_generator,test_generator,current_path)\n",
    "    elif use_type == 'test':\n",
    "        model_directory = \".\\models_KIMORE\\models_KIMORE\\Kimore_ex5.pt\"\n",
    "        net = TemporalModel()\n",
    "        net.load_state_dict(torch.load(model_directory))\n",
    "        print(net)\n",
    "        print(torch.cuda.device_count())\n",
    "#         evaluate(test_generator, data_loader, net)\n",
    "#         print(dataset[0][0])\n",
    "        print(dataset[2][0].shape)\n",
    "    elif use_type == 'inference':\n",
    "        model_directory = \".\\models_KIMORE\\models_KIMORE\\Kimore_ex5.pt\"\n",
    "        net = TemporalModel()\n",
    "        net.load_state_dict(torch.load(model_directory))        \n",
    "        inference(dataset[5][0], dataset[5][1], net, data_loader)\n",
    "    else:\n",
    "        model_directory = \".\\models_KIMORE\\models_KIMORE\\Kimore_ex5.pt\"\n",
    "        net = TemporalModel()\n",
    "        net.load_state_dict(torch.load(model_directory))\n",
    "        print(net.transformer)\n",
    "        return_nodes = {\n",
    "    \"transformer\": \"transformer\"\n",
    "}\n",
    "        \n",
    "        model2 = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "        intermediate_outputs = model2(dataset[5][0])\n",
    "        print(intermediate_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e3cd5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298, 100, 25, 3)\n",
      "(298, 1)\n",
      "(75, 100, 25, 3)\n",
      "(75, 1)\n",
      "Transformer(\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (pos_embedding): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (1): GELU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (pos_embedding): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (1): GELU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (pos_embedding): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (1): GELU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (pos_embedding): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (1): GELU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (pos_embedding): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (1): GELU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (pos_embedding): PositionalEncoding(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): PreNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (1): GELU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a0921b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298, 100, 25, 3)\n",
      "(298, 1)\n",
      "(75, 100, 25, 3)\n",
      "(75, 1)\n",
      "TemporalModel(\n",
      "  (encoder): CurveNet(\n",
      "    (lpfa): LPFA(\n",
      "      (mlp): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(9, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cic11): CIC(\n",
      "      (curveaggregation): CurveAggregation(\n",
      "        (conva): Conv1d(16, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convb): Conv1d(16, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convc): Conv1d(16, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convn): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convl): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convd): Sequential(\n",
      "          (0): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (line_conv_att): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (curvegrouping): CurveGrouping(\n",
      "        (att): Conv1d(16, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (walk): Walk(\n",
      "          (agent_mlp): Sequential(\n",
      "            (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (momentum_mlp): Sequential(\n",
      "            (0): Conv1d(32, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (maxpool): MaskedMaxPool()\n",
      "      (lpfa): LPFA(\n",
      "        (xyz2feature): Sequential(\n",
      "          (0): Conv2d(9, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): LeakyReLU(negative_slope=0.2)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cic12): CIC(\n",
      "      (curveaggregation): CurveAggregation(\n",
      "        (conva): Conv1d(8, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convb): Conv1d(8, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convc): Conv1d(8, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convn): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convl): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convd): Sequential(\n",
      "          (0): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (line_conv_att): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (curvegrouping): CurveGrouping(\n",
      "        (att): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (walk): Walk(\n",
      "          (agent_mlp): Sequential(\n",
      "            (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (momentum_mlp): Sequential(\n",
      "            (0): Conv1d(16, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv1d(32, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(8, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (maxpool): MaskedMaxPool()\n",
      "      (lpfa): LPFA(\n",
      "        (xyz2feature): Sequential(\n",
      "          (0): Conv2d(9, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): LeakyReLU(negative_slope=0.2)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cic21): CIC(\n",
      "      (curveaggregation): CurveAggregation(\n",
      "        (conva): Conv1d(32, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convb): Conv1d(32, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convc): Conv1d(32, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convn): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convl): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convd): Sequential(\n",
      "          (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (line_conv_att): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (curvegrouping): CurveGrouping(\n",
      "        (att): Conv1d(32, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (walk): Walk(\n",
      "          (agent_mlp): Sequential(\n",
      "            (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (momentum_mlp): Sequential(\n",
      "            (0): Conv1d(64, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (maxpool): MaskedMaxPool()\n",
      "      (lpfa): LPFA(\n",
      "        (xyz2feature): Sequential(\n",
      "          (0): Conv2d(9, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): LeakyReLU(negative_slope=0.2)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cic22): CIC(\n",
      "      (curveaggregation): CurveAggregation(\n",
      "        (conva): Conv1d(32, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convb): Conv1d(32, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convc): Conv1d(32, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convn): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convl): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convd): Sequential(\n",
      "          (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (line_conv_att): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (curvegrouping): CurveGrouping(\n",
      "        (att): Conv1d(32, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (walk): Walk(\n",
      "          (agent_mlp): Sequential(\n",
      "            (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (momentum_mlp): Sequential(\n",
      "            (0): Conv1d(64, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (maxpool): MaskedMaxPool()\n",
      "      (lpfa): LPFA(\n",
      "        (xyz2feature): Sequential(\n",
      "          (0): Conv2d(9, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): LeakyReLU(negative_slope=0.2)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv0): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv1d(25, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (time): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (pos_embedding): PositionalEncoding(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "1\n",
      "tensor([0.4481])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabf85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f448c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the size of the data \n",
    "print(Correct_data.shape, 'correct sequences')\n",
    "print(Correct_label.shape, 'correct labels')\n",
    "print(Incorrect_data.shape, 'incorrect sequences')\n",
    "print(Incorrect_label.shape, 'incorrect labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a35254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4451f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
